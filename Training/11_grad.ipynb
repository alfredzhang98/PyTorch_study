{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285edcc4",
   "metadata": {},
   "source": [
    "## 前馈网络\n",
    "\n",
    "![image10.png](mdfiles/image10.png)\n",
    "\n",
    "\n",
    "## 导数\n",
    "\n",
    "$f^{'}  (x_0) = \\lim_{\\Delta x \\to 0} \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}$\n",
    "\n",
    "## 偏导数\n",
    "\n",
    "函数不止有一个变量时，我们需要计算偏导数。\n",
    "\n",
    "z=f(x,y)，当我们要求 x 方向的导数的时候，就可以给 x 一个非常小的增量Δx，同时保持 y 不变。\n",
    "\n",
    "反之，如果要求 y 方向的导数，则需要给 y 一个非常小的增量Δy，而 x 保持不变。于是就能得出如下的偏导数描述公式：\n",
    "\n",
    "\n",
    "$\\frac{\\partial }{\\partial x_j} f(x_1, x_2, ..., x_n) = \\lim_{\\Delta x_j \\to 0} \\frac{f(x_1, x_2, ..., x_j + \\Delta x_j, ..., x_n) - f(x_1, x_2, ..., x_j, ..., x_n)}{\\Delta x_j}$\n",
    "\n",
    "\n",
    "\n",
    "函数所有偏导数构成的向量就叫做梯度\n",
    "\n",
    "__梯度向量的方向即为函数值增长最快的方向__\n",
    "\n",
    "$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5db094",
   "metadata": {},
   "source": [
    "## 链式法则\n",
    "\n",
    "模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。\n",
    "\n",
    "模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。\n",
    "\n",
    "$H(W_{11}, W_{12}, ..., W_{ij}, ..., W_{mn})$\n",
    "\n",
    "\n",
    "$W_{ij}$ 表示模型中的第 i 行第 j 列的权重参数，其梯度向量$\\delta H$ 为：\n",
    "\n",
    "$[\\frac{\\partial H}{\\partial W_{11}}, \\frac{\\partial H}{\\partial W_{12}}, ..., \\frac{\\partial H}{\\partial W_{ij}}, ..., \\frac{\\partial H}{\\partial W_{mn}}]$\n",
    "\n",
    "两个函数组合起来的复合函数，导数等于里面函数代入外函数值的导数，乘以里面函数之导数。\n",
    "\n",
    "$\\frac{dy} {dx} = f^{'}(g(x)) \\cdot g^{'}(x)$\n",
    "\n",
    "$\\frac{dy} {dx} = \\frac{dy} {du} \\cdot \\frac{du} {dx}$\n",
    "\n",
    "### example\n",
    "$f(x) = cos(x^2 - 1)$\n",
    "\n",
    "$f(x) = cos(x)$\n",
    "\n",
    "$g(x) = x^2 - 1$\n",
    "\n",
    "$g(x)$ 的导数为 $g^{'}(x) = 2x$ $f(x)$ 的导数为 $f^{'}(x) = -sin(x)$\n",
    "\n",
    "所以\n",
    "\n",
    "$f^{'}(x) = f^{'}(g(x)) \\cdot g^{'}(x) = -sin(x^2 - 1) \\cdot 2x$\n",
    "\n",
    "\n",
    "具体来说说是换元法，即把复合函数拆成两个函数来求导数，然后再把两个函数的导数相乘即可。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83fac9",
   "metadata": {},
   "source": [
    "__一是通过某种方式反向传播__\n",
    "\n",
    "__二是根据误差不断地调整模型的参数值__\n",
    "\n",
    "通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。\n",
    "\n",
    "在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。\n",
    "\n",
    "优化方法主要是通过梯度下降法来实现的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e49f3",
   "metadata": {},
   "source": [
    "深度学习都是基于反向传播的么？\n",
    "\n",
    "是的，深度学习中的大多数模型训练过程都依赖于反向传播算法。反向传播是一种用于计算神经网络中各个参数梯度的有效方法，它通过链式法则将误差从输出层逐层传递回输入层，从而计算出每个参数对损失函数的贡献。这些梯度信息随后被用来更新模型参数，以最小化损失函数，从而提高模型的性能。因此，反向传播是深度学习训练过程中不可或缺的一部分。它优点是效率高计算速度快，但缺点在于实现起来比较复杂，容易出错。\n",
    "\n",
    "其实还有基于数值微分的方法，但计算量太大，效率太低，一般不会使用。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

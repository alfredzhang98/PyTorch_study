{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1af31b",
   "metadata": {},
   "source": [
    "## 本分类问题有很多经典解决办法\n",
    "\n",
    "- 朴素贝叶斯分类器（Naive Bayes Classifier）通过某些条件发生的概率来进行分类。\n",
    "- 支持向量机（Support Vector Machine, SVM）通过寻找最佳分割超平面来进行分类。\n",
    "- 决策树（Decision Tree）通过树状结构进行分类。\n",
    "- 随机森林（Random Forest）通过集成多个决策树进行分类。\n",
    "- LDA（Latent Dirichlet Allocation）通过主题建模进行分类。\n",
    "- 神经网络（Neural Networks）通过多层感知器进行分类。\n",
    "\n",
    "面临主要问题\n",
    "1. 类别多样性：情感类别多样，可能存在多个情感标签，增加分类难度。\n",
    "2. 语言复杂性：自然语言表达复杂，包含隐含情感、讽刺、双关等，难以准确捕捉情感倾向。\n",
    "3. 数据不平衡：某些情感类别的数据量较少，导致模型难以学习这些类别的特征。\n",
    "4. 上下文依赖性：情感表达往往依赖上下文，单独的句子可能无法准确反映情感倾向。\n",
    "5. 多义词和同义词：词语可能具有多重含义或相似含义，增加情感分类的复杂性。\n",
    "6. 多语言支持：不同语言的情感表达方式不同，增加跨语言情感分类的难度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043ab73",
   "metadata": {},
   "source": [
    "## BERT\n",
    "BERT 采用了基于 MLM 的模型训练方式，即 Mask Language Model。因为 BERT 是 Transformer 的一部分，即 encoder 环节，所以没有 decoder 的部分（其实就是 GPT）。\n",
    "\n",
    "为了解决这个问题，MLM 方式应运而生。它的思想也非常简单，就是在训练之前，随机将文本中一部分的词语（token）进行屏蔽（mask），然后在训练的过程中，使用其他没有被屏蔽的 token 对被屏蔽的 token 进行预测。\n",
    "\n",
    "``` shell\n",
    "wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "unzip multi_cased_L-12_H-768_A-12.zip\n",
    "```\n",
    "### 格式准备\n",
    "\n",
    "- Token embeddings：词向量。这里需要注意的是，Token embeddings 的第一个开头的 token 一定得是“[CLS]”。[CLS]作为整篇文本的语义表示，用于文本分类等任务。\n",
    "- Segment embeddings。这个向量主要是用来将两句话进行区分，比如问答任务，会有问句和答句同时输入，这就需要一个能够区分两句话的操作。不过在咱们此次的分类任务中，只有一个句子。\n",
    "- Position embeddings。位置向量。Transformer 结构中没有 RNN 那样的时序信息，所以需要通过位置向量来表示词语在句子中的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a182faa",
   "metadata": {},
   "source": [
    "### 模型配置\n",
    "\n",
    "id2label：这个字段记录了类别标签和类别名称的映射关系。\n",
    "\n",
    "label2id：这个字段记录了类别名称和类别标签的映射关系。\n",
    "\n",
    "num_labels_cate：类别的数量。数据准备\n",
    "\n",
    "1. InputExample：它用于记录单个训练数据的文本内容的结构。\n",
    "2. DataProcessor：通过这个类中的函数，我们可以将训练数据集的文本，表示为多个 InputExample 组成的数据集合。\n",
    "3. get_features：用于把 InputExample 数据转换成 BERT 能够理解的数据结构的关键函数。我们具体来看一下各个数据都怎么生成的。\n",
    "\n",
    "input_ids 记录了输入 token 对应在 vocab.txt 的 id 序号，它是通过如下的代码得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0dd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceeqz4/.conda/envs/pytorch311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "  torch: 2.5.1+cu121\n",
      "  transformers: 4.57.1\n",
      "  datasets: 4.4.0\n"
     ]
    }
   ],
   "source": [
    "# 环境与依赖（可重复执行）\n",
    "%pip install -q transformers datasets accelerate scikit-learn\n",
    "\n",
    "import torch, transformers, datasets, sklearn\n",
    "print(\"Versions:\")\n",
    "print(\"  torch:\", torch.__version__)\n",
    "print(\"  transformers:\", transformers.__version__)\n",
    "print(\"  datasets:\", datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44319c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330078d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InputExample(guid='imdb-train[:2]-0', text_a='I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', text_b=None, label=0),\n",
       " InputExample(guid='imdb-train[:2]-1', text_a='\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.', text_b=None, label=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# InputExample 与 Processor 定义（IMDB 二分类示例）\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    text_a: str\n",
    "    text_b: Optional[str] = None\n",
    "    label: Optional[int] = None  # 0/1\n",
    "\n",
    "\n",
    "def load_imdb_as_examples(split: str) -> List[InputExample]:\n",
    "    ds = load_dataset(\"imdb\", split=split)\n",
    "    examples = []\n",
    "    for i, ex in enumerate(ds):\n",
    "        guid = f\"imdb-{split}-{i}\"\n",
    "        examples.append(InputExample(guid=guid, text_a=ex[\"text\"], label=int(ex[\"label\"])))\n",
    "    return examples\n",
    "\n",
    "# 演示：取前2个样本看一下结构\n",
    "train_examples_preview = load_imdb_as_examples(\"train[:2]\")\n",
    "train_examples_preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e81ee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23750, 1250, 25000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer、特征化与小批量数据构造\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "max_length = 256\n",
    "\n",
    "_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class BertTextDataset(Dataset):\n",
    "    def __init__(self, examples: List[InputExample]):\n",
    "        self.examples = examples\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        enc = _tokenizer(\n",
    "            ex.text_a,\n",
    "            ex.text_b,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}  # remove batch dim\n",
    "        if ex.label is not None:\n",
    "            item[\"labels\"] = torch.tensor(ex.label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(features: List[Dict[str, torch.Tensor]]):\n",
    "    return _tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# 构建 train/validation/test 划分\n",
    "train_examples = load_imdb_as_examples(\"train[:95%]\")\n",
    "valid_examples = load_imdb_as_examples(\"train[95%:]\")\n",
    "test_examples  = load_imdb_as_examples(\"test\")\n",
    "\n",
    "train_ds = BertTextDataset(train_examples)\n",
    "valid_ds = BertTextDataset(valid_examples)\n",
    "test_ds  = BertTextDataset(test_examples)\n",
    "\n",
    "len(train_ds), len(valid_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4831d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training (BertForSequenceClassification)...\n",
      "[Epoch 1/3] train_loss=0.3022 train_acc=0.8705 | val_loss=0.1987 val_acc=0.9242\n",
      "[Epoch 1/3] train_loss=0.3022 train_acc=0.8705 | val_loss=0.1987 val_acc=0.9242\n",
      "[Epoch 2/3] train_loss=0.1433 train_acc=0.9489 | val_loss=0.1346 val_acc=0.9555\n",
      "[Epoch 2/3] train_loss=0.1433 train_acc=0.9489 | val_loss=0.1346 val_acc=0.9555\n",
      "[Epoch 3/3] train_loss=0.0615 train_acc=0.9811 | val_loss=0.2592 val_acc=0.9148\n",
      "[Epoch 3/3] train_loss=0.0615 train_acc=0.9811 | val_loss=0.2592 val_acc=0.9148\n",
      "[Test] loss=0.2426 acc=0.9230\n",
      "Saved best model to: /home/uceeqz4/Project/learning/PyTorch_study/Application/ckpts/bert_emotion_bertcls\n",
      "[Test] loss=0.2426 acc=0.9230\n",
      "Saved best model to: /home/uceeqz4/Project/learning/PyTorch_study/Application/ckpts/bert_emotion_bertcls\n"
     ]
    }
   ],
   "source": [
    "# 使用 BertForSequenceClassification 的训练代码（纯 PyTorch 训练循环，含 AMP、DataLoader 优化）\n",
    "import os, math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# 目录（Notebook 中使用工作目录）\n",
    "ckpt_dir2 = os.path.join(os.getcwd(), \"ckpts\", \"bert_emotion_bertcls\")\n",
    "os.makedirs(ckpt_dir2, exist_ok=True)\n",
    "\n",
    "# 设备与加速选项\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 允许 TF32（Ampere+ GPU 有效），可提升吞吐\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# AMP 设置（优先使用 bfloat16，否则使用 float16）\n",
    "use_autocast = torch.cuda.is_available()\n",
    "try:\n",
    "    autocast_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "except Exception:\n",
    "    autocast_dtype = torch.float16\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(autocast_dtype == torch.float16 and torch.cuda.is_available()))\n",
    "\n",
    "# DataLoader（多进程、固定内存、预取）\n",
    "num_workers = max(1, min(8, (os.cpu_count() or 2) // 2))\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=16, shuffle=True, collate_fn=collate_batch,\n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=True, prefetch_factor=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds, batch_size=32, shuffle=False, collate_fn=collate_batch,\n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=True, prefetch_factor=2\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_ds,  batch_size=32, shuffle=False, collate_fn=collate_batch,\n",
    "    num_workers=num_workers, pin_memory=True, persistent_workers=True, prefetch_factor=2\n",
    ")\n",
    "print(f\"Using device: {device}, num_workers={num_workers}, amp_dtype={autocast_dtype}\")\n",
    "\n",
    "# 创建模型（加载预训练 BERT）\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 优化器与调度器\n",
    "lr = 2e-5\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 简单准确率\n",
    "def accuracy_from_logits(logits, labels):\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "best_val_loss = math.inf\n",
    "print(\"Start training (BertForSequenceClassification)...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    tr_loss, tr_acc = 0.0, 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_autocast, dtype=autocast_dtype):\n",
    "            out = model(**batch)  # returns SequenceClassifierOutput\n",
    "            loss = out.loss\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        tr_loss += loss.item()\n",
    "        tr_acc  += accuracy_from_logits(out.logits, batch[\"labels\"])    \n",
    "    tr_loss /= len(train_loader)\n",
    "    tr_acc  /= len(train_loader)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    va_loss, va_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            with torch.cuda.amp.autocast(enabled=use_autocast, dtype=autocast_dtype):\n",
    "                out = model(**batch)\n",
    "                va_loss += out.loss.item()\n",
    "                va_acc  += accuracy_from_logits(out.logits, batch[\"labels\"])\n",
    "    va_loss /= len(valid_loader)\n",
    "    va_acc  /= len(valid_loader)\n",
    "\n",
    "    # 保存最优\n",
    "    if va_loss < best_val_loss:\n",
    "        best_val_loss = va_loss\n",
    "        model.save_pretrained(ckpt_dir2)\n",
    "        _tokenizer.save_pretrained(ckpt_dir2)\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{epochs}] train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | val_loss={va_loss:.4f} val_acc={va_acc:.4f}\")\n",
    "\n",
    "# 测试\n",
    "model.eval()\n",
    "te_loss, te_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=use_autocast, dtype=autocast_dtype):\n",
    "            out = model(**batch)\n",
    "            te_loss += out.loss.item()\n",
    "            te_acc  += accuracy_from_logits(out.logits, batch[\"labels\"])  \n",
    "te_loss /= len(test_loader)\n",
    "te_acc  /= len(test_loader)\n",
    "print(f\"[Test] loss={te_loss:.4f} acc={te_acc:.4f}\")\n",
    "\n",
    "print(\"Saved best model to:\", ckpt_dir2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

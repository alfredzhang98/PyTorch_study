{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d996130f",
   "metadata": {},
   "source": [
    "## NLP \n",
    "\n",
    "研究的领域非常广泛，凡是跟语言学有关的内容都属于 NLP 的范畴。\n",
    "\n",
    "一般来说，较为多见的语言学的方向包括：词干提取、词形还原、分词、词性标注、命名实体识别、语义消歧、句法分析、指代消解、篇章分析等方面。\n",
    "\n",
    "\n",
    "![image13.png](mdfiles/image13.png)\n",
    "\n",
    "\n",
    "搜索引擎要对你输入的内容（query）进行解析，这就涉及到了之前提到的分词、命名实体识别、语义消歧等内容，当然还涉及到了 query 纠错，因为你错误地输入了拼音而非汉字，需要改写成正确的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb003c8",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "中文单词和单词之间除了标点符号没有别的隔断。这就给程序理解文本带来了一定的难度，分词的需求也应运而生。\n",
    "\n",
    "深度学习已经对分词的依赖越来越小，可以通过 Word Embedding 等方式对字符（token）级的文字进行表示，但是分词的地位不会降低，单词、词组级别的文本表示仍旧有非常多的应用场景。\n",
    "\n",
    "开源或者免费的 NLP 分词工具，比如 jieba、HanLP、THULAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242c82d",
   "metadata": {},
   "source": [
    "#### jieba 分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb3386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "\u001b[33m  DEPRECATION: Building 'jieba' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'jieba'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=3d9285c1c0bee9cd369d18d9f630e6e1715792dee72675abe68ff489e2a3ca6a\n",
      "  Stored in directory: /home/uceeqz4/.cache/pip/wheels/ac/60/cf/538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a552a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uceeqz4/.conda/envs/pytorch311/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.613 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "极客 时间 棒呆 啦\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "text = \"极客时间棒呆啦\"\n",
    "# jieba.cut得到的是generator形式的结果\n",
    "seg = jieba.cut(text)  \n",
    "print(' '.join(seg)) \n",
    "\n",
    "# Get： 极客 时间 棒呆 啦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair('今年', 't'), pair('天气', 'n'), pair('好', 'a'), pair('晴朗', 'a'), pair('，', 'x'), pair('处处', 'v'), pair('好', 'a'), pair('风光', 'n')]\n"
     ]
    }
   ],
   "source": [
    "# jieba 还提供了词性标注的结果（pos）\n",
    "import jieba.posseg as posseg\n",
    "text = \"今年天气好晴朗，处处好风光\"\n",
    "# 形如pair('word, 'pos')的结果\n",
    "seg = posseg.cut(text)  \n",
    "print([se for se in seg]) \n",
    "\n",
    "# m: 数词\n",
    "# d: 副词\n",
    "# v: 动词\n",
    "# n: 普通名词\n",
    "# r: 代词\n",
    "# a: 形容词\n",
    "# x: 非语素字\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f852ac",
   "metadata": {},
   "source": [
    "#### One-hot\n",
    "\n",
    "假定所有的文字一共有 N 个单词（也可以是字符），我们可以将每个单词赋予一个单独的序号 id，那么对于任意一个单词，我们都可以采用一个 N 位的列表（向量）对其进行表示。在这个表示中，只需要将这个单词对应序号 id 的位置为 1，其他位置为 0 即可。\n",
    "\n",
    "比方说，我们词典大小为 10000，“极客”这个单词的序号 id 为 666，那么我们就需要建立一个 10000 长度的向量\n",
    "\n",
    "\n",
    "在 UTF-8 编码中，中文字符有两万多个，词语数量更是天文数字，那么我们仅用字符的方式，每个字就需要两万多维度的数据来表示。推算一下，如果有一篇一万字的文章，这个数据量就很可怕了。\n",
    "\n",
    "为了进一步的压缩数据的体积，可以只使用一个向量表示文章中所有的单词，例如前面的例子，我们仍旧建立一个 10000 维的向量，把文章中出现过的所有单词的对应位置置为 1，其余为 0。\n",
    "\n",
    "\n",
    "count-based 进一步压缩体积\n",
    "对每一个出现的单词的序号 id 以及出现过的次数进行统计，这样一来，“极客时间”我们只需要两个 k-v 对的 dict 即可表示: {3:1, 665:1}。\n",
    "\n",
    "这种表示方法在 SVM、树模型等多个算法包中被广泛采用，但是新的问题就是不能表述单词的语序信息\n",
    "\n",
    "还有其他很多word Embedding 的方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e84e3",
   "metadata": {},
   "source": [
    "### 关键词的提取\n",
    "\n",
    "基于统计特征的方法\n",
    "\n",
    "TF-IDF（term frequency–inverse document frequency，词频 - 逆向文件频率）\n",
    "\n",
    "一个单词在文件中出现的次数越多，它的重要性越高；但它在语料库中出现的频率越高，它的重要性反而越小。\n",
    "\n",
    "TF-IDF 中，词频（TF）表示关键字在文本中出现的频率。而逆向文件频率 (IDF)  是由包含该词语的文件的数目除以总文件数目得到的，一般情况下还会取对数对结果进行缩放。\n",
    "\n",
    "$$ \n",
    "TF = \\frac{\\text{某词在该文件中出现的次数}}{\\text{该文件的总词数}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "IDF = \\log{\\frac{\\text{总文件数}}{\\text{包含该词的文件数 + 1}}}\n",
    "$$\n",
    "\n",
    "这里为什么分母要加 1 呢？这是为了避免分母为 0 的情况。得到了 TF 和 IDF 之后，我们将两者相乘，就得到了 TF-IDF 了。\n",
    "\n",
    "对单词出现的次数以及分布进行数学上的统计，从而发现相应的规律和重要性（权重），并以此作为关键词提取的依据。\n",
    "\n",
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0346b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 0.3333333333333333, IDF: 0.4054651081081644, TF-IDF: 0.13515503603605478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/uceeqz4/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/uceeqz4/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, TextCollection\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sents = ['i like jike', 'i want to eat apple', 'i like lady gaga']\n",
    "# 分词\n",
    "sents = [word_tokenize(sent) for sent in sents]\n",
    "\n",
    "# 构建语料库\n",
    "corpus = TextCollection(sents)\n",
    "\n",
    "# 选一个文档，比如第一个句子\n",
    "doc = sents[0]\n",
    "\n",
    "# 计算TF、IDF、TF-IDF\n",
    "tf = corpus.tf('like', doc)\n",
    "idf = corpus.idf('like')\n",
    "tf_idf = corpus.tf_idf('like', doc)\n",
    "\n",
    "print(f'TF: {tf}, IDF: {idf}, TF-IDF: {tf_idf}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd777273",
   "metadata": {},
   "source": [
    "| 名称         | 全称                         | 中文含义  | 作用                   |\n",
    "| ---------- | -------------------------- | ----- | -------------------- |\n",
    "| **TF**     | Term Frequency             | 词频    | 表示某个词在一篇文档中出现的“频繁程度” |\n",
    "| **IDF**    | Inverse Document Frequency | 逆文档频率 | 衡量某个词在整个语料库中是否“普遍”   |\n",
    "| **TF-IDF** | TF × IDF                   | 综合权重  | 表示该词在该文档中对“区分主题”的重要性 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da194096",
   "metadata": {},
   "source": [
    "### 基于词图模型的关键词提取\n",
    "\n",
    "基于词图模型的关键词提取。\n",
    "\n",
    "我们首先要构建文本一个图结构，用来表示语言的词语网络。然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，即关键词。\n",
    "\n",
    "TextRank 来自 PageRank 算法\n",
    "\n",
    "PageRank 是 Google 用来对网页进行排序的算法。它的核心思想是：一个网页的重要性由指向它的其他网页的重要性决定。\n",
    "1. 如果一个网页被很多其他网页链接到的话，就说明这个网页比较重要，也就是 PageRank 值会相对较高。\n",
    "2. 如果一个 PageRank 值很高的网页，链接到一个其他的网页，那么被链接到的网页的 PageRank 值会相应地因此而提高。\n",
    "\n",
    "对于TextRank:\n",
    "\n",
    "![image14.png](mdfiles/image14.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605fd284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认词性过滤的关键词:\n",
      "['关键词', '提取', '专注', '程序员', '成长', '主题', '知识', '服务平台', '方法', '包括', '分析', '比如', '应该', '反映', '介绍', '处理', '文本', '进行', '结果', '极客']\n",
      "\n",
      "带权重的关键词:\n",
      "关键词\t1.0000\n",
      "提取\t0.5471\n",
      "专注\t0.3940\n",
      "程序员\t0.3571\n",
      "成长\t0.3541\n",
      "主题\t0.3520\n",
      "知识\t0.3491\n",
      "服务平台\t0.3481\n",
      "方法\t0.3205\n",
      "包括\t0.3187\n",
      "\n",
      "更宽词性集合下的关键词:\n",
      "['关键词', '提取', '专注', '程序员', '成长', '知识', '服务平台', '简单', '应该', '分析']\n"
     ]
    }
   ],
   "source": [
    "# TextRank 示例：从一段中文中抽取关键词\n",
    "from jieba import analyse\n",
    "\n",
    "sentence = (\n",
    "    \"极客时间是一个专注于程序员成长的知识服务平台。本文介绍自然语言处理中的关键词提取方法，\"\n",
    "    \"包括基于统计的TF-IDF和基于图模型的TextRank。我们将使用jieba的TextRank从一段中文里抽取关键词，\"\n",
    "    \"并对结果进行简单分析。关键词应该能反映主题，比如‘关键词提取’、‘文本’、‘算法’、‘图模型’等。\"\n",
    " )\n",
    "\n",
    "# 定义一个较全面的词性集合来模拟“不过滤词性”\n",
    "ALL_POS = (\n",
    "    \"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"nl\",\"ng\",  # 名词及细分\n",
    "    \"v\",\"vd\",\"vn\",\"vq\",\"vg\",            # 动词及细分\n",
    "    \"a\",\"ad\",\"an\",\"ag\",                    # 形容词及细分\n",
    "    \"x\"                                      # 其他（符号/未知）\n",
    " )\n",
    "\n",
    "# 1) 默认（只保留 ns/n/vn/v 这些词性），返回关键词列表\n",
    "keys_default = analyse.textrank(\n",
    "    sentence,\n",
    "    topK=20,\n",
    "    withWeight=False,\n",
    "    allowPOS=(\"ns\", \"n\", \"vn\", \"v\"),\n",
    "    withFlag=False,\n",
    " )\n",
    "print(\"默认词性过滤的关键词:\")\n",
    "print(keys_default)\n",
    "\n",
    "# 2) 返回带权重的关键词 (前10个)\n",
    "keys_weighted = analyse.textrank(\n",
    "    sentence,\n",
    "    topK=10,\n",
    "    withWeight=True,\n",
    "    allowPOS=(\"ns\", \"n\", \"vn\", \"v\"),\n",
    "    withFlag=False,\n",
    " )\n",
    "print(\"\\n带权重的关键词:\")\n",
    "for word, score in keys_weighted:\n",
    "    print(f\"{word}\\t{score:.4f}\")\n",
    "\n",
    "# 3) 使用更全面的词性集合（近似“不过滤”），看看有什么变化\n",
    "keys_allpos = analyse.textrank(\n",
    "    sentence,\n",
    "    topK=10,\n",
    "    withWeight=False,\n",
    "    allowPOS=ALL_POS,   # 用更宽的集合代替 None，避免 TypeError\n",
    "    withFlag=False,\n",
    " )\n",
    "print(\"\\n更宽词性集合下的关键词:\")\n",
    "print(keys_allpos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd99382",
   "metadata": {},
   "source": [
    "### 基于主题模型的关键词提取    \n",
    "\n",
    "LDA（Latent Dirichlet Allocation）文档主题生成模型，是最典型的基于主题模型的算法。通过对文档进行主题建模，可以提取出每个主题的关键词，从而实现对文档的理解和分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b60ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/uceeqz4/.conda/envs/pytorch311/lib/python3.11/site-packages (from gensim) (2.3.3)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.4.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.8 kB)\n",
      "Downloading gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.4.3-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n",
      "Installing collected packages: wrapt, scipy, smart_open, gensim\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [gensim]2m3/4\u001b[0m [gensim]pen]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.4.0 scipy-1.16.3 smart_open-7.4.3 wrapt-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76dc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import jieba.posseg as jp\n",
    "import jieba\n",
    "\n",
    "texts = [line.strip() for line in open('input.txt', 'r')]\n",
    "# 老规矩，先分词\n",
    "words_list = []\n",
    "for text in texts:\n",
    "  words = [w.word for w in jp.cut(text)]\n",
    "  words_list.append(words)\n",
    "\n",
    "# 构建文本统计信息, 遍历所有的文本，为每个不重复的单词分配序列id，同时收集该单词出现的次数\n",
    "dictionary = corpora.Dictionary(words_list)\n",
    "\n",
    "# 构建语料，将dictionary转化为一个词袋。\n",
    "# corpus是一个向量的列表，向量的个数就是文档数。你可以输出看一下它内部的结构是怎样的。\n",
    "corpus = [dictionary.doc2bow(words) for words in words_list]\n",
    "\n",
    "# 开始训练LDA模型\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, num_topics=8, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc764088",
   "metadata": {},
   "source": [
    "在训练环节中，num_topics 代表生成的主题的个数。id2word 即为 dictionary，它把 id 都映射成为字符串。passes 相当于深度学习中的 epoch，表示模型遍历语料库的次数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

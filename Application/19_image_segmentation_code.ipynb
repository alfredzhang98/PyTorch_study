{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de52010f",
   "metadata": {},
   "source": [
    "## 标记是一个很重要的工作\n",
    "\n",
    "https://github.com/wkentaro/labelme\n",
    "\n",
    "1. 需要标记的放在一个文件夹下面\n",
    "2. label.txt 文件，记录类别名称，每行一个类别名称，第一行是背景类，一般不需要标记\n",
    "``` \n",
    "__ignore__\n",
    "_background_\n",
    "cat\n",
    "```\n",
    "3. 使用 **labelme_json_to_dataset** 或者 **label2voc** 工具将标记文件转换为我们需要的格式\n",
    "4. 运行以下脚本自动启动 Labelme\n",
    "```bash\n",
    "labelme --labels labels.txt --nodata\n",
    "```\n",
    "5. 点我们击左侧的 Open Dir 打开图片文件夹，选择一张图片进行标记\n",
    "6. 点击左侧的 Create Polygons Tool 进行标记。当标记完成后，我们需要保存一下，保存之后会生成标记好的 json 文件。\n",
    "7. 执行下面bash脚本，将 json 文件转换为我们需要的 mask 图像 位置在exameple/instance_segmentation/labelme2voc.py\n",
    "```bash\n",
    "python label2voc.py cats cats_output --label label.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c764f2",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "\n",
    "完成了标记工作之后，我们就要用 PyTorch 把这些数据给读入进来了，我们把数据相关的写在 dataset.py 中。\n",
    "\n",
    "具体还是和之前讲的一样，要继承 Dataset 类，然后实现 __init__、__len__ 和 __getitem__ 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecf6b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CatSegmentationDataset(Dataset):\n",
    "    in_channels = 3\n",
    "    out_channels = 1\n",
    "\n",
    "    def __init__(self, images_dir, images_size=32):\n",
    "        print(\"Reading data from:\", images_dir)\n",
    "        image_root_path =  images_dir + os.sep + \"JPEGImages\"\n",
    "        mask_root_path = images_dir + os.sep + \"SegmentationClassPNG\"\n",
    "\n",
    "        self.image_slices = []\n",
    "        self.mask_slices = []\n",
    "\n",
    "        for im_name in os.listdir(image_root_path):\n",
    "            mask_name = im_name.split(\".\")[0] + \".png\"\n",
    "\n",
    "            # os.sep is the system path separator, e.g., '/' for Linux and '\\' for Windows\n",
    "            image_path = image_root_path + os.sep + im_name\n",
    "            mask_path = mask_root_path + os.sep + mask_name\n",
    "\n",
    "            im = np.asarray(Image.open(image_path).resize((images_size, images_size)))\n",
    "            mask = np.asarray(Image.open(mask_path).resize((images_size, images_size)))\n",
    "\n",
    "            self.image_slices.append(im / 255.0)\n",
    "            self.mask_slices.append(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_slices[idx]\n",
    "        mask = self.mask_slices[idx]\n",
    "\n",
    "        image =  image.transpose((2, 0, 1))  # HWC -> CHW\n",
    "        mask = mask[np.newaxis, ...]  # HW -> CHW\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "965b6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loaders(args):\n",
    "    dataset_train = CatSegmentationDataset(\n",
    "        images_dir=args.images,\n",
    "        images_size=args.image_size\n",
    "    )\n",
    "\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "\n",
    "    return loader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11348cba",
   "metadata": {},
   "source": [
    "## 设计模型\n",
    "\n",
    "![image10.png](mdfiles/image10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5066be",
   "metadata": {},
   "source": [
    "1. 图中横向蓝色的箭头，它们都是重复的相同结构，都是由两个 3x3 的卷积层组合而成的，在每层卷积之后会跟随一个 BN 层与 ReLU 的激活层\n",
    "\n",
    "2. 注意绿色箭头的功能，是表示上采样操作，使用的是转置卷积来实现的\n",
    "\n",
    "3. 红色箭头表示max pooling 下采样操作\n",
    "\n",
    "4. 最后输出层使用是二分类因此使用 1x1 的卷积将通道数变为 2 ！！！\n",
    "\n",
    "5. 灰色表示跳跃连接，将编码器部分的特征图与解码器部分对应尺寸的特征图进行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fb558",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "Dice Loss\n",
    "\n",
    "Dice 系数，常用于计算两个集合的相似度，取值范围在 0-1 之间\n",
    "\n",
    "$$\n",
    "\\text { Dice }=\\frac{2|P \\cap G|}{|P|+|G|}\n",
    "$$\n",
    "\n",
    "GT 只有 0 和 1 两个值。当我们直接使用模型输出的预测概率而不是使用阈值将它们转换为二值 Mask 时，这种损失函数就被称为 Soft Dice Loss。此时，∣P∩G∣ 的值近似为 GT 与预测概率矩阵的点乘。\n",
    "\n",
    "根据 Dice 系数我们就能设计出一种损失函数，也就是 Dice Loss。它的计算公式非常简单，如下所示。\n",
    "\n",
    "$$\n",
    "\\text { Dice Loss }=1-\\text { Dice }\n",
    "$$\n",
    "\n",
    "当预测值的 Mask 与 GT 越相似，损失就越小；当预测值的 Mask 与 GT 差异度越大，损失就越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "627474a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = float(smooth)\n",
    "\n",
    "        if class_weights is not None:\n",
    "            w = torch.tensor(class_weights, dtype=torch.float32)\n",
    "            self.register_buffer(\"class_weights\", w)\n",
    "        else:\n",
    "            self.register_buffer(\"class_weights\", None)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # ones = torch.ones(1, device=y_pred.device, dtype=y_pred.dtype)\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "304a1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from unet import UNet\n",
    "\n",
    "\n",
    "def makedirs(args):\n",
    "    os.makedirs(args.ckpts, exist_ok=True)\n",
    "    os.makedirs(args.logs, exist_ok=True)\n",
    "\n",
    "\n",
    "def audit_devices(model):\n",
    "    bad = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.device.type == \"cpu\":\n",
    "            bad.append((\"param\", name, tuple(p.shape), p.device))\n",
    "    for name, b in model.named_buffers():\n",
    "        if b.device.type == \"cpu\":\n",
    "            bad.append((\"buffer\", name, tuple(b.shape), b.device))\n",
    "    return bad\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    makedirs(args)\n",
    "    device = torch.device(\"cpu\" if not torch.cuda.is_available() else args.device)\n",
    "\n",
    "    loader_train = data_loaders(args)\n",
    "\n",
    "    unet = UNet(in_channels=CatSegmentationDataset.in_channels, out_channels=CatSegmentationDataset.out_channels)\n",
    "    unet = unet.to(device)\n",
    "\n",
    "    problems = audit_devices(unet)\n",
    "    if problems:\n",
    "        print(\"Still on CPU before forward():\")\n",
    "        for kind, name, shape, dev in problems:\n",
    "            print(f\" - {kind:6s} {name:40s} {shape} @ {dev}\")\n",
    "    else:\n",
    "        print(\"All registered params/buffers moved to\", device)\n",
    "\n",
    "    dsc_loss = DiceLoss().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(unet.parameters(), lr=args.lr)\n",
    "\n",
    "    loss_train = []\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs), total=args.epochs):\n",
    "        unet.train()\n",
    "        for batch_idx, (data, target) in enumerate(loader_train):\n",
    "            step += 1\n",
    "            data   = torch.as_tensor(data, device=device)\n",
    "            target = torch.as_tensor(target, device=device)\n",
    "\n",
    "            y_pred = unet(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = dsc_loss(y_pred, target)\n",
    "\n",
    "            loss_train.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step) % 10 == 0:\n",
    "                print('Step', step, 'Training Loss:', np.mean(loss_train[-10:]))\n",
    "\n",
    "        torch.save(unet, args.ckpts + '/unet_epoch_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56b1c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: ./data/work2\n",
      "All registered params/buffers moved to cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__cudnn_batch_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m args = argparse.Namespace(\n\u001b[32m      3\u001b[39m     batch_size=\u001b[32m16\u001b[39m,      \u001b[38;5;66;03m# Batch Size\u001b[39;00m\n\u001b[32m      4\u001b[39m     epochs=\u001b[32m10\u001b[39m,         \u001b[38;5;66;03m# Epoch number\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     image_size=\u001b[32m256\u001b[39m     \u001b[38;5;66;03m# target input image size\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 运行训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     50\u001b[39m data   = torch.as_tensor(data, device=device)\n\u001b[32m     51\u001b[39m target = torch.as_tensor(target, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m y_pred = \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m optimizer.zero_grad()\n\u001b[32m     55\u001b[39m loss = dsc_loss(y_pred, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/learning/PyTorch_study/Application/unet/unet.py:87\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     conv_encoder_1_1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_encoder_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# max pooling 下采样 kernel_size=2, stride=2\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# kernel_size=2 means we take 2x2 window to do max pooling\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# stride=2 means we move the window 2 pixels each step\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# resulting feature map size will be halved\u001b[39;00m\n\u001b[32m     92\u001b[39m     conv_encoder_1_2 = nn.MaxPool2d(kernel_size=\u001b[32m2\u001b[39m, stride=\u001b[32m2\u001b[39m)(conv_encoder_1_1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/learning/PyTorch_study/Application/unet/unet.py:26\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m     25\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     x = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBatchNorm2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     x = nn.ReLU(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)(x)\n\u001b[32m     28\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch311/lib/python3.11/site-packages/torch/nn/functional.py:2812\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2810\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2812\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2813\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__cudnn_batch_norm)"
     ]
    }
   ],
   "source": [
    "# 在 Notebook 中使用 argparse 不方便，这里直接构造一个默认参数对象\n",
    "args = argparse.Namespace(\n",
    "    batch_size=16,      # Batch Size\n",
    "    epochs=10,         # Epoch number\n",
    "    lr=0.0001,         # Learning rate\n",
    "    device=\"cuda:0\",   # Device for training (auto-falls back to CPU if no CUDA)\n",
    "    num_workers=4,     # Dataloader workers\n",
    "    ckpts=\"./ckpts/work2\",   # folder to save weights\n",
    "    logs=\"./logs\",     # folder to save logs\n",
    "    images=\"./data/work2\",   # root folder with images (should contain JPEGImages & SegmentationClassPNG)\n",
    "    image_size=256     # target input image size\n",
    ")\n",
    "\n",
    "# 运行训练\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e97d89",
   "metadata": {},
   "source": [
    "## 几个地方需要注意换到device上\n",
    "\n",
    "1. model = model.to(device=device)\n",
    "2. data   = data.to(device=device, non_blocking=True)\n",
    "3. target = target.to(device=device, non_blocking=True)\n",
    "4. loss = loss.to(device=device)\n",
    "\n",
    "注意在网络不建议使用没有在init里面初始化的操作放在forward里面\n",
    "\n",
    "使用 import torch.nn.functional as F, 然后在 forward 里面使用 F.xxx() 的方式调用 例如 F.relu()  F.softmax()  F.conv2d() F.max_pool2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab58ff",
   "metadata": {},
   "source": [
    "## 评价指标\n",
    "\n",
    "常用的评价指标是 mIoU。mIoU 全称为 mean Intersection over Union，即平均交并比。交并比是真实值和预测值的交集和并集之比。\n",
    "\n",
    "![image12.png](mdfiles/image12.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad7001",
   "metadata": {},
   "source": [
    "## 新图片测试\n",
    "\n",
    "没学过的果然不行\n",
    "\n",
    "![output_overlay.png](output_overlay.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
